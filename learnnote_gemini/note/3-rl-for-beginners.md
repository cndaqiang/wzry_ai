# 第三章：强化学习入门：从监督学习到 DQN

对于有监督学习背景的开发者来说，强化学习（RL）既熟悉又陌生。本章将尝试为你搭建一座从监督学习通往强化学习的桥梁，核心是弄明白：**模型到底在学习什么，以及它是如何学习的**。

## 3.1 强化学习的目标：与监督学习的损失函数作类比

让我们先回顾一下你熟悉的监督学习（以图像分类为例）：

-   **输入**：一张图片 (`image`)。
-   **模型**：一个 CNN 网络。
-   **输出**：一个预测标签 (`predicted_label`)，比如 "猫"。
-   **目标**：我们有一个真实的标签 `true_label`（"猫"）。
-   **损失函数**：`loss = Loss(predicted_label, true_label)`，例如交叉熵损失。
-   **学习过程**：通过反向传播，调整模型权重，让 `predicted_label` 尽可能地接近 `true_label`。

现在，我们来看看强化学习（DQN）：

-   **输入**：一个游戏画面 (`state`)。
-   **模型**：一个 DQN 网络。
-   **输出**：当前状态下，**每个可能动作的“长期价值”预测**，我们称之为 `Q` 值。例如 `[Q(向左), Q(向右), Q(攻击), ...]`。
-   **目标**：我们没有一个直接的 `true_label` 告诉我们哪个动作是“正确”的。但我们可以构造一个“目标Q值”。
-   **损失函数**：`loss = MSELoss(预测的Q值, 目标Q值)`。
-   **学习过程**：通过反向传播，调整模型权重，让“预测的Q值”尽可能地接近“目标Q值”。

**核心区别**：监督学习的“正确答案”是数据集中固有的。而强化学习的“正确答案”（目标Q值）是**根据环境反馈（奖励）和模型自身的预测，在每一步动态构造出来的**。这个构造公式就是著名的 **Bellman 方程** 的简化应用：

`目标Q值 = 即时奖励 + gamma * 未来最乐观的Q值`

这个公式是后续所有内容的核心，我们会在下面反复看到它。

## 3.2 `dqnAgent.py` 剖析：智能体 (Agent) 是如何做出决策的？

`DQNAgent` 就是 AI 玩家。它的核心职责之一是在每个时刻决定执行哪个动作。这个决策过程在 `select_action` 函数中。

```python
# dqnAgent.py
def select_action(self, state):
    # 产生一个 0 到 1 的随机数
    rand = np.random.rand()
    
    # epsilon 是“探索率”，一个逐渐变小的数
    if rand <= self.epsilon:
        # (A) 探索：完全随机选择一个动作
        return [np.random.randint(size) for size in self.action_sizes]
    else:
        # (B) 利用：让模型根据当前经验做出最优决策
        self.policy_net.eval()
        with torch.no_grad():
            # 将游戏画面 state 喂给模型
            q_values = self.policy_net(tmp_state_640_640)
        # 对模型输出的每个动作分支，选择 Q 值最大的那个动作
        return [np.argmax(q.detach().cpu().numpy()) for q in q_values]
```

这里体现了 RL 的一个核心概念：**探索（Exploration） vs. 利用（Exploitation）**。

-   **利用 (B)**：相信模型当前的判断，选择它认为 Q 值最高的动作。这就像一个学生用他已经学会的知识来解题。
-   **探索 (A)**：完全无视模型的建议，随机尝试。这就像学生偶尔不按套路出牌，去尝试新的解法，也许能发现更好的路径。

`epsilon` 参数就是这两者之间的平衡器。我们将在 `3.4` 节详细讨论它。

## 3.3 Q值、奖励和损失：模型到底在学习什么？

`replay` 函数是 AI 的“学习”过程，也是最难理解的部分。我们把它拆解来看，并与代码一一对应。

**学习目标**：让模型预测的 `Q(s, a)`（在 `s` 状态下执行 `a` 动作的长期价值）逼近 `r + γ * max(Q(s', a'))`（即时奖励 `r` 加上折扣后的未来最大价值）。

```python
# dqnAgent.py -> replay()

# 1. 准备数据：从经验池随机抽取一批 (s, a, r, s', done)
transitions = globalInfo.random_batch_size_memory_dqn()
batch = Transition(*zip(*transitions))
# ... 将数据转换为 Tensor ...

# 2. 计算“预测的Q值”
#    将一批次的状态 s 喂给 policy_net 模型
state_action_values = self.policy_net(batch_state)
#    模型输出了所有动作的Q值，我们只关心当时实际执行的那个动作 a 的Q值
#    .gather() 就是用来精确提取这些Q值的
state_action_q_values = move_action_q.gather(1, batch_action[:, 0].unsqueeze(1)) + ...

# 3. 计算“目标Q值”
#    首先，用 target_net 模型预测下一状态 s' 的所有动作的Q值
next_state_action_values = self.target_net(non_final_next_states)
#    然后，在每个动作分支上，我们只取其中最大的那个Q值（最乐观的估计）
next_state_values[non_final_mask] = torch.max(next_move_action_q, 1)[0] + ...
#    最后，根据 Bellman 方程，构造目标Q值
#    注意：(1 - batch_done) 保证了如果游戏结束 (done=1)，未来价值就为 0
expected_state_action_values = batch_reward + self.gamma * next_state_values * (1 - batch_done)

# 4. 计算损失
#    就是我们熟悉的均方误差损失
loss = self.criterion(state_action_q_values, expected_state_action_values.unsqueeze(1))

# 5. 优化模型
#    标准的反向传播流程
self.optimizer.zero_grad()
loss.backward()
self.optimizer.step()
```

**总结一下**：`replay` 函数的本质就是：
1.  拿出一条过去的经验 `(s, a, r, s')`。
2.  问模型：“你当初觉得在 `s` 状态下执行 `a` 动作能得多少分？” -> **预测Q值**。
3.  自己算一下：“根据实际拿到的奖励 `r` 和 `s'` 状态的潜力，当初那个动作应该值多少分？” -> **目标Q值**。
4.  比较这两个分数的差距，如果模型当初预测高了或低了，就让它根据这个差距 `loss` 进行修正。

## 3.4 `epsilon` 参数：为什么有时要“随机乱玩”而不是永远“听模型的”？

`epsilon` 是探索率。你可以把它想象成 AI 的“**好奇心**”或“**自信心**”的度量。

-   **训练初期 (`epsilon` 接近 1.0)**：模型什么都不懂，像个无头苍蝇。此时让它“随机乱玩”（高探索率）是最好的选择，因为这样可以收集到五花八门的经验，看到世界的更多可能性。如果一开始就完全听模型的，它可能会因为偶然一次的成功而“固执地”重复某个无效动作。

-   **训练过程 (`epsilon` 衰减)**：
    ```python
    if self.epsilon > self.epsilon_min:
        self.epsilon *= self.epsilon_decay
    ```
    随着训练的进行，模型学到的知识越来越多，变得越来越“自信”。因此，我们通过 `epsilon_decay` 逐步降低它的“好奇心”，让它更多地“利用”已经学到的成功经验。

-   **训练后期 (`epsilon` 达到 `epsilon_min`)**：即使模型已经很强了，我们仍然保留一点点“好奇心”（`epsilon_min`），以防万一存在模型从未发现过的更优策略。

这个从高到低的 `epsilon` 变化过程，完美地平衡了“广泛搜集信息”和“利用最优策略”这两个阶段。

## 3.5 新手常见问题：用“随机”数据来训练一个“最优”模型，这合理吗？

这正是你提出的那个非常深刻的问题。答案是：**合理，而关键就在于 `ReplayMemory`（经验回放池）**。

如果我们的学习方式是：玩一步 -> 学习一步 -> 玩下一步 -> 学习下一步...
那么问题就很大。因为早期的动作是随机的，状态是连续的，模型会基于一连串“垃圾数据”进行学习，很容易学偏。

但 `ReplayMemory` 打破了这个魔咒。它有两个关键作用：

1.  **打破时间相关性**：经验池存储了成千上万条来自不同时间的经验。每次训练时，我们是**随机**从中抽取一个 `batch`。这个 `batch` 里可能包含了 1 分钟前的成功攻击、30 秒前的无效移动和 5 秒前的惊险逃生。模型学习的是一个打乱了时序的、“去相关性”的数据集，这和监督学习中 `shuffle` 数据集的道理是一样的，能防止模型被短期内的连续烂操作带偏。

2.  **经验的重复利用**：一条成功的经验（比如一次击杀获得了高奖励）会被存储在经验池中，并在后续的训练中被**反复抽取和学习**。这使得模型能充分吸收这些“高光时刻”的价值，而不是昙花一现就忘了。

所以，虽然数据来源包含了大量随机探索，但**经验回放机制**保证了模型是从一个宏观的、多样化的、包含好坏经验的“大数据集”中学习，从而能够去伪存真，逐渐逼近最优策略。

## 3.6 新手开发经验与避坑指南

1.  **奖励函数是灵魂**：
    *   **问题**: 模型“挂机”不动或者只会“反复横跳”。
    *   **原因**: 很可能是你的奖励函数出了问题。比如，你只在游戏胜利时给一个大奖励，在其他时间都给 0 或 -1。模型很难把几十分钟前的一个正确移动和最终的胜利联系起来。
    *   **建议**: **奖励要尽可能即时和稠密**。
        *   敌人血条减少 -> 正奖励。
        *   我方血条减少 -> 负奖励。
        *   向敌方水晶前进 -> 小小的正奖励。
        *   原地不动太久 -> 小小的负奖励。
        *   一个好的奖励函数能极大加速收敛。

2.  **先简化，再复杂**：
    *   **问题**: 一开始就想训练一个能放所有技能、走位风骚的完美 AI，结果完全不收敛。
    *   **原因**: 动作空间太大，状态太复杂，模型难以学习。
    *   **建议**:
        *   **简化动作空间**：初期可以只让模型学习移动，比如只输出一个移动角度。攻击、技能等可以暂时用固定逻辑（如“检测到敌人就自动普攻”）。
        *   **简化状态**：可以先把游戏画面转为灰度图，或者降低分辨率，减少模型需要处理的信息量。
        *   先用一个简单的模型验证整个流程（数据采集、训练、更新）是通的，再逐步增加复杂度。

3.  **学会看日志**：
    *   **`loss`**: `loss` 是最重要的指标。它不下降，说明模型没在学习。如果它变成 `NaN`，说明梯度爆炸了，可能学习率太大或奖励值设置得过大。
    *   **`epsilon`**: 打印 `epsilon`，确保它在按预期衰减。
    *   **`reward`**: 打印每一步获得的 `reward`，检查奖励函数是否在按你的设想工作。
    *   **Q 值**: 偶尔可以打印一下模型输出的 Q 值，看看它们大概在什么范围，是否在逐渐增大。

4.  **超参数是门玄学**：
    *   不要指望一次就能找到完美的超参数组合。
    *   **控制变量法**：一次只调整一个参数（比如 `learning_rate`），观察其对 `loss` 曲线的影响。
    *   如果显存允许，`batch_size` 和 `memory_size` 通常是越大越好。
    *   `gamma` 通常不需要频繁调整，`0.99` 在大部分游戏中都是一个不错的选择。
