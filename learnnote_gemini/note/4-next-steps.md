# 第四章：下一步：二次开发与新项目规划

在深入理解了现有代码的结构和原理之后，我们就可以基于您的想法，规划如何进行二次开发，以及如何构建一个更健壮、更灵活的新项目。

## 4.1 整合现有思路：从 `myideal.txt` 和 `README.md` 出发

您在 `myideal.txt` 和 `README.md` 中已经提出了非常有价值的改进方向，我们将其总结和提炼如下：

1.  **核心交互层替换**：
    *   **痛点**：当前项目依赖 `scrcpy` 窗口截图，限制了后台运行；同时 `onnx` 模型的环境配置较为繁琐。
    *   **解决方案**：使用您自己的 `autowzry` 库作为底层。
    *   **优势**：
        *   **后台运行**：`autowzry` 基于 `airtest`，可以直接与后台的安卓模拟器或真实设备进行通信，不再需要一个前台窗口。
        *   **环境简化**：可以移除 `onnxruntime` 和 `ppocronnx` 的依赖，让环境配置更轻松。
        *   **状态判断**：可以直接复用 `autowzry` 中已经实现的游戏状态判断逻辑（如判断是否在对局中），无需依赖 ONNX 模型。

2.  **训练目标简化**：
    *   **痛点**：当前模型的动作空间非常复杂（包含移动、攻击、技能、信号等 8 个维度），导致模型难以学习和收敛。
    *   **解决方案**：**初期只专注于训练移动**。让模型只决定“是否移动”以及“朝哪个方向移动”。
    *   **优势**：
        *   **降低学习难度**：动作空间急剧减小，模型可以更快地学到有效的移动策略。
        *   **高复用性**：一个好的移动策略对于大多数英雄都是通用的。其他操作（如攻击、放技能）可以暂时交由 `autowzry` 的自动化脚本来处理（例如，进入攻击范围就自动攻击）。

3.  **探索监督学习方案**：
    *   **思路**：除了强化学习，还可以尝试监督学习。通过录制真人玩家或内置 AI 的对局，收集大量的 `(游戏画面, 实际移动方向)` 数据对。
    *   **优势**：
        *   **更简单直接**：这变成了一个标准的图像分类/回归问题，对于有监督学习经验的你来说，上手更快，调试也更直观。
        *   **作为“启动模型”**：可以用监督学习预训练一个“懂点事”的基础模型，然后再用强化学习对它进行微调，这通常比从零开始的强化学习要快得多。

这些思路为我们的新项目 `autowzry-agent` 指明了清晰的方向。

## 4.2 新项目技术选型：为什么用 `autowzry`？模型如何从简到繁？

**技术栈核心**：

-   **设备交互与环境**：`autowzry`。这是新项目的基石，负责所有与游戏本体的通信。
-   **机器学习框架**：`PyTorch`。业界主流，社区活跃，生态完善。
-   **配置管理**：`PyYAML`。使用 `.yaml` 文件来管理所有超参数和路径配置，比 `argparse.py` 更结构化，更易于管理。

**模型迭代路径（从简到繁）**：

1.  **阶段一：验证流程 (Sanity Check)**
    *   **模型**：一个极简的 CNN 网络，甚至可以只是几层全连接层（`nn.Linear`）。
    *   **动作空间**：极度简化，比如只输出 4 个离散动作（上、下、左、右）。
    *   **目标**：不求效果多好，只为验证整个数据流（`autowzry` 截图 -> 模型预测 -> `autowzry` 执行 -> 计算奖励 -> 存入经验池 -> 模型训练）是完全通畅的。这是最重要的一步。

2.  **阶段二：有效移动**
    *   **模型**：使用一个轻量级的 CNN（如 3-4 个卷积层），类似于当前项目中的 `net_actor.py`。
    *   **动作空间**：输出一个连续的角度（0-359度），或者 8 个、16 个离散的方向。
    *   **目标**：训练出一个能根据小地图、敌我位置等信息，做出合理移动决策的模型。例如，懂得朝兵线前进，懂得在低血量时后撤。

3.  **阶段三：集成更多动作**
    *   **模型**：在阶段二的基础上，增加新的输出分支，用于控制“是否释放技能”、“是否普通攻击”等。
    *   **动作空间**：逐步扩展，引入技能和攻击的决策。
    *   **目标**：在学会走位的基础上，学习简单的战斗技巧。

这个循序渐进的过程，能让你在每个阶段都获得正反馈，并更容易定位问题。

## 4.3 推荐的项目代码结构（附说明）

一个良好规划的项目结构能让开发事半功倍。

```
autowzry-agent/
├── configs/
│   └── base_config.yaml      # 存放所有超参数、路径等配置
│
├── main.py                     # 主入口：读取配置，启动训练
│
├── agent/
│   ├── base_agent.py           # 定义 Agent 的抽象基类（包含 save, load, select_action, learn 等接口）
│   └── dqn_agent.py            # 具体的 DQN Agent 实现，继承自 BaseAgent
│
├── env/
│   └── wzry_env.py             # 环境封装。它内部调用 autowzry，并为 Agent 提供统一的 step 和 reset 接口
│
├── models/
│   └── simple_cnn.py           # 神经网络结构定义
│
├── training/
│   ├── train_rl.py             # 强化学习的训练主循环
│   └── train_supervised.py     # (可选) 监督学习的训练主循环
│
└── utils/
    ├── replay_memory.py        # 经验回放池
    └── logger.py               # (可选) 用于记录 loss、reward 等训练日志的工具
```

**结构说明**：

-   **`configs/`**: 将所有可变配置（学习率、模型路径、设备ID等）都放在这里。这样做的好处是，你想改变实验参数时，只需修改 `yaml` 文件，而无需触碰代码。
-   **`main.py`**: 保持主入口的简洁。它的唯一任务就是解析配置文件，然后把任务分发给 `training` 目录下的相应脚本。
-   **`agent/`**: 将 Agent 的逻辑（决策+学习）和模型（`models/`）本身分离。Agent 负责“如何使用模型”，模型只负责“前向传播计算”。
-   **`env/`**: 这是适配层。`wzry_env.py` 将 `autowzry` 的各种函数（如 `截图()`、`移动()`）包装成符合 OpenAI Gym 风格的 `step(action)` 接口。这样做的好处是，未来如果想换成别的游戏环境，只需重写 `env` 目录，而 `agent` 和 `training` 的代码几乎不用动。
-   **`training/`**: 将不同的训练范式（强化学习、监督学习）分离开，使代码逻辑更清晰。

这个结构遵循了“高内聚、低耦合”的原则，每一部分各司其职，非常适合迭代开发和未来的功能扩展。
