# 第一章：代码结构与执行流程

本章旨在梳理整个项目的宏观结构，让你明白每个文件是做什么的，以及它们是如何组合在一起，让整个训练流程运转起来的。

## 1.1 各个 Python 文件的作用

-   **`train.py`**: **项目总指挥**。
    这是你运行整个程序的入口。它负责启动数据采集和模型训练这两个核心线程，并协调它们的工作。

-   **`dqnAgent.py`**: **AI 的“大脑”**。
    `DQNAgent` 类封装了强化学习的核心逻辑。它包含了神经网络（`policy_net` 和 `target_net`），并负责两件大事：1. 根据当前游戏画面选择下一步动作（`select_action`）；2. 从经验池中学习如何做出更好的决策（`replay`）。

-   **`net_actor.py`**: **神经网络的“设计图”**。
    这里定义了 `NetDQN` 类的具体结构，即神经网络的层级关系。它描述了输入的图片（state）经过卷积层和全连接层，最终如何变成一系列动作的 Q 值。

-   **`wzry_env.py`**: **游戏“环境”的模拟器**。
    `Environment` 类将 AI 的动作（如移动、攻击）转化为对安卓设备的具体操作，并返回操作后的结果（新的游戏画面、奖励等）。它扮演了 AI 与游戏世界之间的桥梁。

-   **`android_tool.py`**: **安卓设备“遥控器”**。
    `AndroidTool` 类提供了所有与安卓设备交互的底层工具，例如截图 (`screenshot_window`) 和执行 ADB 命令来模拟点击和滑动 (`action_move`, `action_attack`)。

-   **`getReword.py`**: **“裁判员”**。
    `GetRewordUtil` 类负责根据游戏画面计算奖励（Reward）。例如，通过 `cv2` 识别敌人血条的减少来判断我方攻击是否有效，或者通过 `ppocronnx` 识别“胜利/失败”文字来给予最终的大奖或大罚。

-   **`memory.py`**: **“记忆”存储单元**。
    定义了 `ReplayMemory` 类，这是一个固定容量的经验池。AI 在游戏中的每一步操作及其结果（state, action, reward, next_state, done）都会被作为一个 `Transition` 存放在这里。

-   **`globalInfo.py`**: **“全局公告板”**。
    使用 `@singleton` 模式创建了一个全局唯一的 `GlobalInfo` 实例，用于存放所有线程共享的信息，比如经验池 `dqn_memory` 和游戏状态 `start_game`。

-   **`argparses.py`**: **“配置中心”**。
    这里集中管理了所有可以从命令行调整的参数（超参数），如学习率、`epsilon` 值、`batch_size` 等，方便你调整实验配置而无需修改代码。

-   **`onnxRunner.py`**: **ONNX 模型“运行器”**。
    一个工具类，用于加载和运行 `.onnx` 格式的模型。在这个项目中，它主要被用来运行 `death.onnx` 和 `start.onnx`，以快速判断对局是否开始或角色是否死亡。

-   **`showposition.py`**: **“坐标拾取器”**。
    一个基于 `PyQt5` 的图形界面小工具，用于加载截图并方便地获取屏幕上任意点的坐标，这在配置 `argparses.py` 中的点击位置时非常有用。

## 1.2 程序如何从启动到训练：一步步解析

当你运行 `python train.py` 时，程序内部发生了以下事情：

1.  **初始化**：
    *   程序加载所有必要的模块，并根据 `argparses.py` 创建配置。
    *   `agent = DQNAgent()` 创建了 AI 大脑。
    *   `tool = AndroidTool()` 创建了手机遥控器。
    *   `globalInfo = GlobalInfo()` 创建了全局公告板，里面的经验池 `dqn_memory` 还是空的。

2.  **启动两个核心线程**：
    *   `train.py` 的 `main` 函数最后启动了两个并行的线程：
        *   `training_thread = threading.Thread(target=train_agent)`：**训练师线程**。
        *   `data_collector()`：**采集工函数**（在主线程中运行）。

3.  **两个线程开始并行工作**：

    *   **采集工 (`data_collector`)**：
        *   它在一个 `while True` 循环里不停地工作。
        *   **步骤 A**: 调用 `tool.screenshot_window()` 截取当前游戏画面 (`state`)。
        *   **步骤 B**: 判断游戏是否开始。在本项目中，它使用 `start_check.get_max_label(state)` 来识别画面中的“开战”标识。
        *   **步骤 C (如果游戏已开始)**:
            1.  调用 `action = agent.select_action(state)`，让 AI 大脑根据当前画面决定一个动作。
            2.  调用 `env.step(action)`，通过遥控器执行动作，并获取动作后的新画面 `next_state` 和奖励 `reward`。
            3.  调用 `globalInfo.store_transition_dqn(...)`，将这次经历 `(state, action, reward, next_state, done)` 存入全局公告板的经验池中。
            4.  更新 `state = next_state`，准备下一次决策。
            5.  如果 `done=1`（游戏结束），则跳出循环，回到步骤 B 等待下一局。

    *   **训练师 (`train_agent`)**：
        *   它也在一个 `while True` 循环里。
        *   **步骤 X**: 它会先检查 `globalInfo.is_memory_bigger_batch_size_dqn()`，即经验池里的数据够不够一个批次 (`batch_size`)。
        *   **步骤 Y (如果数据足够)**:
            1.  调用 `agent.replay()`，这是最核心的训练步骤。AI 大脑会从经验池中随机抽取一批数据，然后进行一次梯度下降，更新神经网络的权重。
            2.  训练完成后，它会继续回到步骤 X，检查数据量，然后再次训练。

**总结**：你可以把这个过程想象成一个“边玩边学”的学生。“采集工”是他正在玩游戏的手和眼，不断地尝试、犯错、获取经验，并把所有经历都写在“错题本”（经验池）上。“训练师”是他在课后复习的大脑，不断地从“错题本”中翻看过去的经验，总结规律，提升自己的游戏水平（更新模型）。这两个过程同时进行，使得 AI 能够持续地自我提升。

## 1.3 `argparses.py` 中的超参数：它们是什么，如何影响训练

超参数是在开始学习过程之前设置值的参数，而不是通过训练得到的参数数据。调整它们是强化学习中最“玄学”也最重要的一部分。

-   **`learning_rate` (学习率)**
    *   **是什么**：控制模型每次更新权重的幅度。可以理解为模型学习的“步子大小”。
    *   **如何影响**：
        *   **太大**：模型可能在最优解附近“反复横跳”，导致 `loss` 难以收敛。
        *   **太小**：模型学习得非常慢，需要很长时间才能看到效果。
    *   **新手建议**：通常从 `0.001` 或 `0.0001` 开始尝试。

-   **`gamma` (折扣因子)**
    *   **是什么**：衡量未来奖励相对于即时奖励的重要性的参数。取值范围在 0 和 1 之间。
    *   **如何影响**：
        *   **接近 1 (如 0.99)**：模型更有“远见”，会为了未来的一个大奖励而放弃眼前的蝇头小利。这通常是好的，但可能导致学习周期变长。
        *   **接近 0 (如 0.8)**：模型更“短视”，只关心下一步能拿到的奖励。这会让模型快速学会一些简单动作，但可能学不会长期策略。
    *   **新手建议**：对于游戏 AI，一个有远见的模型通常更好，所以 `0.9` 到 `0.99` 是常用范围。

-   **`epsilon`, `epsilon_decay`, `epsilon_min` (探索率相关)**
    *   **`epsilon` (初始探索率)**：在训练开始时，有多大的概率会随机选择一个动作，而不是听从模型的建议。
    *   **`epsilon_decay` (探索率衰减)**：每一步训练后，`epsilon` 会乘以这个值。因为它小于 1，所以 `epsilon` 会逐渐变小。
    *   **`epsilon_min` (最小探索率)**：`epsilon` 不会无限变小，它会保持在这个最小值之上，确保即使在训练后期，AI 仍然有微小的可能性去探索新的未知策略。
    *   **如何影响**：这个组合控制了从“探索”（Exploration）到“利用”（Exploitation）的转变过程。前期高 `epsilon` 保证了 AI 能体验到足够多样的情况；后期低 `epsilon` 保证了 AI 能稳定地使用它学到的最优策略。
    *   **新手建议**：`epsilon` 从 `1.0` 开始，`epsilon_decay` 设为 `0.995` ~ `0.999`，`epsilon_min` 设为 `0.01` 或 `0.1` 是一个不错的起点。

-   **`batch_size` (批处理大小)**
    *   **是什么**：每次调用 `replay()` 时，从经验池中抽取的样本数量。
    *   **如何影响**：
        *   **太大**：每次更新权重考虑的经验更多，梯度方向更稳定，但训练速度变慢，且非常消耗显存。
        *   **太小**：训练速度快，但每次更新的梯度可能波动很大，训练过程不稳定。
    *   **新手建议**：`32`, `64`, `128` 是常见值。可以根据你的显存大小来选择。

-   **`memory_size` (经验池容量)**
    *   **是什么**：`ReplayMemory` 最多能存储多少条经验。
    *   **如何影响**：容量越大，经验的多样性越好，模型学习到的策略可能更具泛化性。但同样，它会占用更多的内存。
    *   **新手建议**：`10000` 是一个不错的开始。如果内存足够，可以设得更大。
